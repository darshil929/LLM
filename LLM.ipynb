{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57LNOSCKhW_m",
        "outputId": "844f9fa4-6250-482b-b126-692b1b104a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.9)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.24)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.26 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.27)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.9)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.26->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.26->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain) (3.9.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass(\"HF token\")"
      ],
      "metadata": {
        "id": "QM_S5FXvpP9k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d16e72b0-1027-4087-8057-afb0fe8527db"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF token··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id = \"google/flan-t5-xxl\", temperature = 0.5, max_new_tokens = 512\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihkQgj0PpZ58",
        "outputId": "a5f49166-567f-4be7-d3a3-6facfc5b9fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
        "Their superior performance over smaller models has made them incredibly\n",
        "useful for developers building NLP enabled applications. These models\n",
        "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
        "using the `openai` library, and via Cohere using the `cohere` library.\n",
        "\n",
        "Question: Which libraries and model providers offer LLMs?\n",
        "\n",
        "Answer: \"\"\""
      ],
      "metadata": {
        "id": "1z46Lfdjpok0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_response = llm(prompt)\n",
        "print(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZDlUas6s6jR",
        "outputId": "aacff20e-0c82-44db-b51f-57c9873d524d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face's transformers library, via OpenAI using the openai library, and via Cohere using the cohere library\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
        "Their superior performance over smaller models has made them incredibly\n",
        "useful for developers building NLP enabled applications. These models\n",
        "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
        "using the `openai` library, and via Cohere using the `cohere` library.\n",
        "\n",
        "Question: Name some prominent celebrities that were invited for Ayodhya temple inauguration recently in 2024.\n",
        "\n",
        "Answer: \"\"\""
      ],
      "metadata": {
        "id": "HL1RoP5ps-S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_response = llm(prompt)\n",
        "print(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMspffzvt9Bs",
        "outputId": "f81566b7-832e-4b11-8e65-da3808977da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(input_variables=[\"query\", \"context\"],\n",
        "                                 template = template)"
      ],
      "metadata": {
        "id": "o0O0pbXgt_w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myprompt = prompt_template.format(\n",
        "        query=\"Which libraries and model providers offer LLMs?\",\n",
        "        context=\"Large Language Models (LLMs) are the latest models used in NLP. Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. These models can be accessed via Hugging Face's `transformers` library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\"\n",
        "    )\n",
        "print(myprompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpOthX8GucSI",
        "outputId": "64fe3fd7-46ae-4ba2-f97f-8fc2a4c59da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the question based on the context below. If the\n",
            "question cannot be answered using the information provided answer\n",
            "with \"I don't know\".\n",
            "\n",
            "Context: Large Language Models (LLMs) are the latest models used in NLP. Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. These models can be accessed via Hugging Face's `transformers` library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\n",
            "\n",
            "Question: Which libraries and model providers offer LLMs?\n",
            "\n",
            "Answer: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm(myprompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nDcTLq7ufOv",
        "outputId": "22bed94b-45cc-4fe0-d2e5-39eeb2f1b0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face's transformers library, via OpenAI using the openai library, and via Cohere using the cohere library\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt_template,llm=llm)\n",
        "query=\"Which libraries and model providers offer LLMs?\"\n",
        "context=\"Large Language Models (LLMs) are the latest models used in NLP. Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. These models can be accessed via Hugging Face's `transformers` library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\"\n",
        "\n",
        "print(llm_chain.run({\"query\": query, \"context\": context}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "purAFpIyuwqT",
        "outputId": "2eb95148-f99c-498e-f9c8-1eb1190bbd4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face's transformers library, via OpenAI using the openai library, and via Cohere using the cohere library\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "ZonV101FuzNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "aTBPlEqCvmXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who is your creator?\"\n",
        "print(question)\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fplDQfkkyong",
        "outputId": "b4029c6e-4e01-4529-d1f3-50d42f59f9d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who is your creator?\n",
            "God is a supreme being. God created the universe and everything in it. The universe and everything in it were created by God. The answer: God.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who is the CEO of LangChain open source framework?\"\n",
        "print(question)\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZt5Z9iLyzIC",
        "outputId": "6bb43f61-168a-4ce0-b4c9-b847381f0434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who is the CEO of LangChain open source framework?\n",
            "LangChain is an open source framework for building distributed ledgers. The CEO of LangChain is David Michaud. The answer: David Michaud.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is 2*5?\"\n",
        "print(question)\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twk3i0-2y_oJ",
        "outputId": "c08cd940-2f0b-4b5a-ac48-20e9472495a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is 2*5?\n",
            "2 * 5 is 5 * 2. The answer: 5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is 2**5?\"\n",
        "print(question)\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1XvKsFj0HN6",
        "outputId": "138d8304-ffda-4a9c-ed37-82578fd2ea60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is 2**5?\n",
            "2**5 is the product of 2 and 5. The answer: 5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import ConversationChain\n",
        "conversation = ConversationChain(llm=llm,verbose=True)"
      ],
      "metadata": {
        "id": "u66aIu4T0J7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Alice has a parrot.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "u2Y_eMO50ZfQ",
        "outputId": "e97cc024-4a65-44e7-eb5f-2b48f429b0b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Alice has a parrot.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I think that is a parrot.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Bob has two cats.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "iwAJD1kp0caN",
        "outputId": "9ae8a207-71b0-46de-bf1f-8f5e846f723b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Alice has a parrot.\n",
            "AI: I think that is a parrot.\n",
            "Human: Bob has two cats.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I think that is a cat.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"How many pets do Alice and Bob have?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "w9f6LkjW0hPs",
        "outputId": "bf4b8492-f86f-4dc1-c884-9a4c2e9eb0a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Alice has a parrot.\n",
            "AI: I think that is a parrot.\n",
            "Human: Bob has two cats.\n",
            "AI: I think that is a cat.\n",
            "Human: How many pets do Alice and Bob have?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'They have three pets.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    }
  ]
}